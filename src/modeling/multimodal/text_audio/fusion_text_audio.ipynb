{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6cde40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Path penting\n",
    "DATA_PATH = r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\data\\training\\multimodal_splits\\text_audio_dataset.csv\"\n",
    "AUDIO_EMB_PATH = \"audio_embeddings_precomputed.npz\"\n",
    "AUDIO_HEAD_PATH = r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\models\\audio_baseline\\best_audio_wav2vec2.pt\"\n",
    "TEXT_MODEL_PATH = r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\models\\text_baseline\\indobert-base-p1\"  \n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b8a1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sample_id label     data_source  confidence  sample_weight  \\\n",
      "0  YT_00081  hoax  predicted_hoax    0.905624       0.820156   \n",
      "1  YT_00867  hoax  predicted_hoax    0.850997       0.724195   \n",
      "2  YT_00904  hoax  predicted_hoax    0.772077       0.596103   \n",
      "3  YT_01161  hoax  predicted_hoax    0.755714       0.571103   \n",
      "4  YT_00039  hoax  predicted_hoax    0.711433       0.506137   \n",
      "\n",
      "                                               title  \\\n",
      "0  Publik Geram, Hukum Tumpul di Kasus Silfester?...   \n",
      "1  Wakil Presiden Gibran Rakabuming Raka Digugat ...   \n",
      "2  Peran dan Ideologi Partai Politik di Indonesia...   \n",
      "3  [BREAKING NEWS] Sederet Nama yang Dilantik Men...   \n",
      "4  Apakah Benar Jokowi Jadi Sekjen PBB 2026? | On...   \n",
      "\n",
      "                                        text_content  \\\n",
      "0  Publik Geram, Hukum Tumpul di Kasus Silfester?...   \n",
      "1  Wakil Presiden Gibran Rakabuming Raka Digugat ...   \n",
      "2  Peran dan Ideologi Partai Politik di Indonesia...   \n",
      "3  [BREAKING NEWS] Sederet Nama yang Dilantik Men...   \n",
      "4  Apakah Benar Jokowi Jadi Sekjen PBB 2026? | On...   \n",
      "\n",
      "                              audio_path                       domain  \\\n",
      "0  ./data/raw/youtube/audio/YT_00081.wav                    METRO TV    \n",
      "1  ./data/raw/youtube/audio/YT_00867.wav  Banjarmasin Post News Video   \n",
      "2  ./data/raw/youtube/audio/YT_00904.wav             scientialoquendi   \n",
      "3  ./data/raw/youtube/audio/YT_01161.wav                   tvOneNews    \n",
      "4  ./data/raw/youtube/audio/YT_00039.wav                   tvOneNews    \n",
      "\n",
      "       date  \n",
      "0  20250807  \n",
      "1  20250903  \n",
      "2  20250702  \n",
      "3  20251107  \n",
      "4  20251015  \n",
      "Distribusi label: label_int\n",
      "1    156\n",
      "0     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Asumsikan ada kolom: sample_id, title, label, audio_path_full\n",
    "print(df.head())\n",
    "\n",
    "label_map = {\"hoax\": 0, \"valid\": 1}\n",
    "df[\"label_int\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "print(\"Distribusi label:\", df[\"label_int\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c00dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 145 label distrib:\n",
      " label_int\n",
      "1    109\n",
      "0     36\n",
      "Name: count, dtype: int64\n",
      "Val size: 31 label distrib:\n",
      " label_int\n",
      "1    23\n",
      "0     8\n",
      "Name: count, dtype: int64\n",
      "Test size: 32 label distrib:\n",
      " label_int\n",
      "1    24\n",
      "0     8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df[\"label_int\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df[\"label_int\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "for name, part in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    print(name, \"size:\", len(part), \"label distrib:\\n\", part[\"label_int\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d79989",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(TEXT_MODEL_PATH)\n",
    "text_model = BertForSequenceClassification.from_pretrained(TEXT_MODEL_PATH).to(DEVICE)\n",
    "text_model.eval()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row[\"title\"])\n",
    "        label = int(row[\"label_int\"])\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "        return item\n",
    "\n",
    "def text_collate_fn(batch):\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n",
    "    attn = torch.stack([b[\"attention_mask\"] for b in batch])\n",
    "    labels = torch.stack([b[\"label\"] for b in batch])\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attn, \"label\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7e43f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embedding: 208\n"
     ]
    }
   ],
   "source": [
    "data_npz = np.load(AUDIO_EMB_PATH, allow_pickle=True)\n",
    "embeddings_np = data_npz[\"embeddings\"]\n",
    "ids_np = data_npz[\"ids\"]  # jika di v4 diisi sample_id, bagus\n",
    "paths_np = data_npz[\"paths\"]\n",
    "\n",
    "print(\"Total embedding:\", len(embeddings_np))\n",
    "\n",
    "# buat mapping sample_id -> index embedding\n",
    "id2idx = {sid: i for i, sid in enumerate(ids_np)}\n",
    "\n",
    "class AudioEmbDataset(Dataset):\n",
    "    def __init__(self, df, id2idx, embeddings_np):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.id2idx = id2idx\n",
    "        self.embeddings_np = embeddings_np\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sid = str(row[\"sample_id\"])  # pastikan cocok dengan ids_np\n",
    "        label = int(row[\"label_int\"])\n",
    "\n",
    "        if sid in self.id2idx:\n",
    "            e = self.embeddings_np[self.id2idx[sid]]\n",
    "        else:\n",
    "            # jika tidak ada embedding (harusnya jarang): isi nol\n",
    "            e = np.zeros((1, 768), dtype=np.float32)\n",
    "\n",
    "        e_t = torch.tensor(e, dtype=torch.float32)  # [T, D] atau [1, D]\n",
    "        return {\"emb\": e_t, \"label\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "def audio_collate_fn(batch):\n",
    "    embs = [b[\"emb\"] for b in batch]\n",
    "    labels = torch.stack([b[\"label\"] for b in batch])\n",
    "\n",
    "    # pad ke panjang sama di dim T\n",
    "    max_len = max(e.shape[0] for e in embs)\n",
    "    dim = embs[0].shape[1]\n",
    "    padded = torch.zeros(len(embs), max_len, dim, dtype=torch.float32)\n",
    "    for i, e in enumerate(embs):\n",
    "        padded[i, :e.shape[0], :] = e\n",
    "\n",
    "    return {\"emb\": padded, \"label\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce4671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text loaders\n",
    "train_text_ds = TextDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_text_ds   = TextDataset(val_df, tokenizer, MAX_LEN)\n",
    "test_text_ds  = TextDataset(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "train_text_loader = DataLoader(train_text_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=text_collate_fn)\n",
    "val_text_loader   = DataLoader(val_text_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=text_collate_fn)\n",
    "test_text_loader  = DataLoader(test_text_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=text_collate_fn)\n",
    "\n",
    "# Audio loaders\n",
    "train_audio_ds = AudioEmbDataset(train_df, id2idx, embeddings_np)\n",
    "val_audio_ds   = AudioEmbDataset(val_df, id2idx, embeddings_np)\n",
    "test_audio_ds  = AudioEmbDataset(test_df, id2idx, embeddings_np)\n",
    "\n",
    "train_audio_loader = DataLoader(train_audio_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=audio_collate_fn)\n",
    "val_audio_loader   = DataLoader(val_audio_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=audio_collate_fn)\n",
    "test_audio_loader  = DataLoader(test_audio_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=audio_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8acdf6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_27864\\983985502.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_head.load_state_dict(torch.load(AUDIO_HEAD_PATH, map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioHeadV4(\n",
       "  (pool): AttentivePool(\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (context): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definisi head sama persis dengan audio_onlyv4\n",
    "\n",
    "class AttentivePool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.context = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):          # x: [B, T, D]\n",
    "        h = torch.tanh(self.linear(x))\n",
    "        scores = self.context(h).squeeze(-1)   # [B, T]\n",
    "        w = torch.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        return (w * x).sum(dim=1)              # [B, D]\n",
    "\n",
    "class AudioHeadV4(nn.Module):\n",
    "    def __init__(self, dim=768, hidden=256, nclass=2):\n",
    "        super().__init__()\n",
    "        self.pool = AttentivePool(dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, nclass),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):          # x: [B, T, D]\n",
    "        pooled = self.pool(x)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "audio_head = AudioHeadV4(dim=768, hidden=256, nclass=2).to(DEVICE)\n",
    "audio_head.load_state_dict(torch.load(AUDIO_HEAD_PATH, map_location=DEVICE))\n",
    "audio_head.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c82b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_probs(loader):\n",
    "    all_probs = []\n",
    "    text_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
    "            outputs = text_model(input_ids=input_ids, attention_mask=attn)\n",
    "            logits = outputs.logits\n",
    "            probs = softmax(logits, dim=-1)[:, 0]  # prob kelas hoax (label 0)\n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "    return np.array(all_probs)\n",
    "\n",
    "\n",
    "def get_audio_probs(loader):\n",
    "    all_probs = []\n",
    "    audio_head.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            emb = batch[\"emb\"].to(DEVICE)       # [B, T, D]\n",
    "            logits = audio_head(emb)\n",
    "            probs = softmax(logits, dim=-1)[:, 0]  # prob hoax\n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "    return np.array(all_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a701b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_text_train = get_text_probs(train_text_loader)\n",
    "p_text_val   = get_text_probs(val_text_loader)\n",
    "p_text_test  = get_text_probs(test_text_loader)\n",
    "\n",
    "p_audio_train = get_audio_probs(train_audio_loader)\n",
    "p_audio_val   = get_audio_probs(val_audio_loader)\n",
    "p_audio_test  = get_audio_probs(test_audio_loader)\n",
    "\n",
    "y_train = train_df[\"label_int\"].to_numpy()\n",
    "y_val   = val_df[\"label_int\"].to_numpy()\n",
    "y_test  = test_df[\"label_int\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40cdef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha_text       acc   f1_hoax  f1_valid  f1_macro\n",
      "0          0.0  0.806452  0.666667  0.863636  0.765152\n",
      "1          0.1  0.903226  0.800000  0.936170  0.868085\n",
      "2          0.2  0.838710  0.615385  0.897959  0.756672\n",
      "3          0.3  0.806452  0.500000  0.880000  0.690000\n",
      "4          0.4  0.838710  0.615385  0.897959  0.756672\n",
      "5          0.5  0.806452  0.571429  0.875000  0.723214\n",
      "6          0.6  0.774194  0.533333  0.851064  0.692199\n",
      "7          0.7  0.774194  0.533333  0.851064  0.692199\n",
      "8          0.8  0.774194  0.533333  0.851064  0.692199\n",
      "9          0.9  0.774194  0.533333  0.851064  0.692199\n",
      "10         1.0  0.774194  0.533333  0.851064  0.692199\n",
      "Best alpha_text: 0.1\n"
     ]
    }
   ],
   "source": [
    "def evaluate_probs(p_hoax, y_true, thr=0.5):\n",
    "    y_pred = (p_hoax >= thr).astype(int)   # 1 = hoax? (disesuaikan)\n",
    "    # HATI-HATI: label_int kita 0=hoax,1=valid â†’ kita mau prediksi 0\n",
    "    # Lebih aman: definisikan y_pred=0 jika prob hoax>=thr, else 1\n",
    "    y_pred = np.where(p_hoax >= thr, 0, 1)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_hoax = f1_score(y_true, y_pred, pos_label=0)\n",
    "    f1_valid = f1_score(y_true, y_pred, pos_label=1)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return acc, f1_hoax, f1_valid, f1_macro\n",
    "\n",
    "def fusion_weighted(p_text, p_audio, alpha):\n",
    "    return alpha * p_text + (1 - alpha) * p_audio\n",
    "\n",
    "alphas = np.linspace(0, 1, 11)\n",
    "records = []\n",
    "for a in alphas:\n",
    "    p_fuse_val = fusion_weighted(p_text_val, p_audio_val, a)\n",
    "    acc, f1h, f1v, f1m = evaluate_probs(p_fuse_val, y_val)\n",
    "    records.append((a, acc, f1h, f1v, f1m))\n",
    "\n",
    "df_alpha = pd.DataFrame(records, columns=[\"alpha_text\", \"acc\", \"f1_hoax\", \"f1_valid\", \"f1_macro\"])\n",
    "print(df_alpha)\n",
    "best_row = df_alpha.sort_values(\"f1_macro\", ascending=False).iloc[0]\n",
    "best_alpha = float(best_row[\"alpha_text\"])\n",
    "print(\"Best alpha_text:\", best_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2be2fb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Results ===\n",
      "Text only  : acc=0.7188 f1h=0.4000 f1v=0.8163 f1m=0.6082\n",
      "Audio only : acc=0.8125 f1h=0.6250 f1v=0.8750 f1m=0.7500\n",
      "Fusion     : acc=0.8125 f1h=0.5714 f1v=0.8800 f1m=0.7257\n",
      "\n",
      "Classification report (fusion):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.5000    0.5714         8\n",
      "           1     0.8462    0.9167    0.8800        24\n",
      "\n",
      "    accuracy                         0.8125        32\n",
      "   macro avg     0.7564    0.7083    0.7257        32\n",
      "weighted avg     0.8013    0.8125    0.8029        32\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 4  4]\n",
      " [ 2 22]]\n"
     ]
    }
   ],
   "source": [
    "# Text-only dan audio-only sebagai pembanding\n",
    "acc_t, f1h_t, f1v_t, f1m_t = evaluate_probs(p_text_test, y_test)\n",
    "acc_a, f1h_a, f1v_a, f1m_a = evaluate_probs(p_audio_test, y_test)\n",
    "\n",
    "# Fusion\n",
    "p_fuse_test = fusion_weighted(p_text_test, p_audio_test, best_alpha)\n",
    "acc_f, f1h_f, f1v_f, f1m_f = evaluate_probs(p_fuse_test, y_test)\n",
    "\n",
    "print(\"=== Test Results ===\")\n",
    "print(\"Text only  : acc={:.4f} f1h={:.4f} f1v={:.4f} f1m={:.4f}\".format(acc_t, f1h_t, f1v_t, f1m_t))\n",
    "print(\"Audio only : acc={:.4f} f1h={:.4f} f1v={:.4f} f1m={:.4f}\".format(acc_a, f1h_a, f1v_a, f1m_a))\n",
    "print(\"Fusion     : acc={:.4f} f1h={:.4f} f1v={:.4f} f1m={:.4f}\".format(acc_f, f1h_f, f1v_f, f1m_f))\n",
    "\n",
    "print(\"\\nClassification report (fusion):\")\n",
    "y_pred_f = np.where(p_fuse_test >= 0.5, 0, 1)\n",
    "print(classification_report(y_test, y_pred_f, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86849ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved late fusion model to text_audio_late_fusion_alpha0_1.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "class TextAudioLateFusion:\n",
    "    def __init__(self, alpha_text=0.1):\n",
    "        self.alpha_text = float(alpha_text)\n",
    "        self.alpha_audio = 1.0 - float(alpha_text)\n",
    "\n",
    "    def predict_proba(self, p_text, p_audio):\n",
    "        \"\"\"\n",
    "        p_text : array shape (N,2) atau (N,) prob hoax\n",
    "        p_audio: array shape (N,2) atau (N,) prob hoax\n",
    "        return : probs_fusion shape (N,2)\n",
    "        \"\"\"\n",
    "        if p_text.ndim == 2:\n",
    "            p_text_hoax = p_text[:, 0]\n",
    "        else:\n",
    "            p_text_hoax = p_text\n",
    "\n",
    "        if p_audio.ndim == 2:\n",
    "            p_audio_hoax = p_audio[:, 0]\n",
    "        else:\n",
    "            p_audio_hoax = p_audio\n",
    "\n",
    "        p_hoax = self.alpha_text * p_text_hoax + self.alpha_audio * p_audio_hoax\n",
    "        p_valid = 1.0 - p_hoax\n",
    "        return np.vstack([p_hoax, p_valid]).T\n",
    "\n",
    "    def predict(self, p_text, p_audio):\n",
    "        probs = self.predict_proba(p_text, p_audio)\n",
    "        # 0 = hoax, 1 = valid\n",
    "        return (probs[:, 0] < 0.5).astype(int)\n",
    "\n",
    "# bikin instance dengan alpha terbaik\n",
    "fusion_model = TextAudioLateFusion(alpha_text=best_alpha)\n",
    "\n",
    "# simpan ke file\n",
    "joblib.dump(fusion_model, r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\models\\fusion_final/text_audio_late_fusion_alpha0_1.joblib\")\n",
    "print(\"Saved late fusion model to text_audio_late_fusion_alpha0_1.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
