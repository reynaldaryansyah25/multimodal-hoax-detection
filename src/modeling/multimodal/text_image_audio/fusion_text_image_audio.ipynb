{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069d4d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\data\\training\\multimodal_splits\\text_image_audio_dataset.csv\"\n",
    "\n",
    "TEXT_MODEL_PATH = r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\models\\text_baseline\\indobert-base-p1\"\n",
    "IMAGE_MODEL_PATH = r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\models\\image_baseline\\best_mobilenetv3_tf_style.pth\"\n",
    "AUDIO_HEAD_PATH = r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\models\\audio_baseline\\best_audio_wav2vec2.pt\"\n",
    "AUDIO_EMB_PATH = \"audio_embeddings_precomputed.npz\"\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1d3659d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sample_id label     data_source  confidence  sample_weight  \\\n",
      "0  YT_00081  hoax  predicted_hoax    0.905624       0.820156   \n",
      "1  YT_00867  hoax  predicted_hoax    0.850997       0.724195   \n",
      "2  YT_00904  hoax  predicted_hoax    0.772077       0.596103   \n",
      "3  YT_01161  hoax  predicted_hoax    0.755714       0.571103   \n",
      "4  YT_00039  hoax  predicted_hoax    0.711433       0.506137   \n",
      "\n",
      "                                               title  \\\n",
      "0  Publik Geram, Hukum Tumpul di Kasus Silfester?...   \n",
      "1  Wakil Presiden Gibran Rakabuming Raka Digugat ...   \n",
      "2  Peran dan Ideologi Partai Politik di Indonesia...   \n",
      "3  [BREAKING NEWS] Sederet Nama yang Dilantik Men...   \n",
      "4  Apakah Benar Jokowi Jadi Sekjen PBB 2026? | On...   \n",
      "\n",
      "                                        text_content  \\\n",
      "0  Publik Geram, Hukum Tumpul di Kasus Silfester?...   \n",
      "1  Wakil Presiden Gibran Rakabuming Raka Digugat ...   \n",
      "2  Peran dan Ideologi Partai Politik di Indonesia...   \n",
      "3  [BREAKING NEWS] Sederet Nama yang Dilantik Men...   \n",
      "4  Apakah Benar Jokowi Jadi Sekjen PBB 2026? | On...   \n",
      "\n",
      "                                    image_path  \\\n",
      "0  ./data/raw/youtube/thumbnails/YT_00081.webp   \n",
      "1   ./data/raw/youtube/thumbnails/YT_00867.jpg   \n",
      "2   ./data/raw/youtube/thumbnails/YT_00904.jpg   \n",
      "3   ./data/raw/youtube/thumbnails/YT_01161.jpg   \n",
      "4   ./data/raw/youtube/thumbnails/YT_00039.jpg   \n",
      "\n",
      "                              audio_path                       domain  \\\n",
      "0  ./data/raw/youtube/audio/YT_00081.wav                    METRO TV    \n",
      "1  ./data/raw/youtube/audio/YT_00867.wav  Banjarmasin Post News Video   \n",
      "2  ./data/raw/youtube/audio/YT_00904.wav             scientialoquendi   \n",
      "3  ./data/raw/youtube/audio/YT_01161.wav                   tvOneNews    \n",
      "4  ./data/raw/youtube/audio/YT_00039.wav                   tvOneNews    \n",
      "\n",
      "       date  label_int  \n",
      "0  20250807          0  \n",
      "1  20250903          0  \n",
      "2  20250702          0  \n",
      "3  20251107          0  \n",
      "4  20251015          0  \n",
      "Total data: 208\n",
      "Label distrib total:\n",
      " label_int\n",
      "1    156\n",
      "0     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "label_map = {\"hoax\": 0, \"valid\": 1}\n",
    "df[\"label_int\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "print(df.head())\n",
    "print(\"Total data:\", len(df))\n",
    "print(\"Label distrib total:\\n\", df[\"label_int\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48cc2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 145\n",
      "label_int\n",
      "1    109\n",
      "0     36\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Val size: 31\n",
      "label_int\n",
      "1    23\n",
      "0     8\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Test size: 32\n",
      "label_int\n",
      "1    24\n",
      "0     8\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,               \n",
    "    stratify=df[\"label_int\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,              \n",
    "    stratify=temp_df[\"label_int\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "for name, part in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    print(f\"{name} size: {len(part)}\")\n",
    "    print(part[\"label_int\"].value_counts(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dfcafb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(TEXT_MODEL_PATH)\n",
    "text_model = BertForSequenceClassification.from_pretrained(TEXT_MODEL_PATH).to(DEVICE)\n",
    "text_model.eval()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row[\"title\"])   # atau \"text_content\" jika ingin\n",
    "        label = int(row[\"label_int\"])\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def text_collate_fn(batch):\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n",
    "    attn = torch.stack([b[\"attention_mask\"] for b in batch])\n",
    "    labels = torch.stack([b[\"label\"] for b in batch])\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attn, \"label\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5500b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio embeddings: 208\n"
     ]
    }
   ],
   "source": [
    "data_npz = np.load(AUDIO_EMB_PATH, allow_pickle=True)\n",
    "embeddings_np = data_npz[\"embeddings\"]\n",
    "ids_np = data_npz[\"ids\"]\n",
    "\n",
    "print(\"Total audio embeddings:\", len(embeddings_np))\n",
    "\n",
    "# mapping sample_id -> index embedding\n",
    "id2idx = {str(sid): i for i, sid in enumerate(ids_np)}\n",
    "\n",
    "class AudioEmbDataset(Dataset):\n",
    "    def __init__(self, df, id2idx, embeddings_np):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.id2idx = id2idx\n",
    "        self.embeddings_np = embeddings_np\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sid = str(row[\"sample_id\"])\n",
    "        label = int(row[\"label_int\"])\n",
    "\n",
    "        if sid in self.id2idx:\n",
    "            e = self.embeddings_np[self.id2idx[sid]]\n",
    "        else:\n",
    "            e = np.zeros((1, 768), dtype=np.float32)\n",
    "\n",
    "        e_t = torch.tensor(e, dtype=torch.float32)  # [T, D] atau [1, D]\n",
    "        return {\"emb\": e_t, \"label\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "def audio_collate_fn(batch):\n",
    "    embs = [b[\"emb\"] for b in batch]\n",
    "    labels = torch.stack([b[\"label\"] for b in batch])\n",
    "\n",
    "    max_len = max(e.shape[0] for e in embs)\n",
    "    dim = embs[0].shape[1]\n",
    "    padded = torch.zeros(len(embs), max_len, dim, dtype=torch.float32)\n",
    "    for i, e in enumerate(embs):\n",
    "        padded[i, :e.shape[0], :] = e\n",
    "\n",
    "    return {\"emb\": padded, \"label\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7fb61cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform disesuaikan dengan training mobilenetv3_tf_style\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # ganti kalau di model_gambar pakai mean/std lain\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, img_size=(224, 224)):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform or (lambda x: x)\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        rel_path = row[\"image_path\"]         \n",
    "        img_path = os.path.join(\n",
    "            r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\", \n",
    "            rel_path.lstrip(\"./\\\\\")                              \n",
    "        )\n",
    "        label = int(row[\"label_int\"])\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, OSError):\n",
    "            img = Image.new(\"RGB\", self.img_size, color=(128, 128, 128))\n",
    "\n",
    "        img_t = self.transform(img)\n",
    "        return {\"image\": img_t, \"label\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "\n",
    "def image_collate_fn(batch):\n",
    "    imgs = torch.stack([b[\"image\"] for b in batch])\n",
    "    labels = torch.stack([b[\"label\"] for b in batch])\n",
    "    return {\"image\": imgs, \"label\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "60208c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text loaders\n",
    "train_text_ds = TextDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_text_ds   = TextDataset(val_df, tokenizer, MAX_LEN)\n",
    "test_text_ds  = TextDataset(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "train_text_loader = DataLoader(train_text_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=text_collate_fn)\n",
    "val_text_loader   = DataLoader(val_text_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=text_collate_fn)\n",
    "test_text_loader  = DataLoader(test_text_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=text_collate_fn)\n",
    "\n",
    "# Audio loaders\n",
    "train_audio_ds = AudioEmbDataset(train_df, id2idx, embeddings_np)\n",
    "val_audio_ds   = AudioEmbDataset(val_df, id2idx, embeddings_np)\n",
    "test_audio_ds  = AudioEmbDataset(test_df, id2idx, embeddings_np)\n",
    "\n",
    "train_audio_loader = DataLoader(train_audio_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=audio_collate_fn)\n",
    "val_audio_loader   = DataLoader(val_audio_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=audio_collate_fn)\n",
    "test_audio_loader  = DataLoader(test_audio_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=audio_collate_fn)\n",
    "\n",
    "# Image loaders (base_dir bisa diisi root folder gambar kalau di CSV relatif)\n",
    "train_img_ds = ImageDataset(train_df, transform=img_transform)\n",
    "val_img_ds   = ImageDataset(val_df,   transform=img_transform)\n",
    "test_img_ds  = ImageDataset(test_df,  transform=img_transform)\n",
    "\n",
    "\n",
    "train_img_loader = DataLoader(train_img_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=image_collate_fn)\n",
    "val_img_loader   = DataLoader(val_img_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=image_collate_fn)\n",
    "test_img_loader  = DataLoader(test_img_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=image_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9f1a6b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21400\\2184449448.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_state = torch.load(AUDIO_HEAD_PATH, map_location=DEVICE)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21400\\2184449448.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  img_state = torch.load(IMAGE_MODEL_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Audio head (harus sama definisi dgn audio_onlyv4)\n",
    "\n",
    "class AttentivePool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.context = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):          # x: [B, T, D]\n",
    "        h = torch.tanh(self.linear(x))\n",
    "        scores = self.context(h).squeeze(-1)   # [B, T]\n",
    "        w = torch.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        return (w * x).sum(dim=1)              # [B, D]\n",
    "\n",
    "\n",
    "class AudioHeadV4(nn.Module):\n",
    "    def __init__(self, dim=768, hidden=256, nclass=2):\n",
    "        super().__init__()\n",
    "        self.pool = AttentivePool(dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, nclass),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):          # x: [B, T, D]\n",
    "        pooled = self.pool(x)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "\n",
    "audio_head = AudioHeadV4(dim=768, hidden=256, nclass=2).to(DEVICE)\n",
    "audio_state = torch.load(AUDIO_HEAD_PATH, map_location=DEVICE)\n",
    "audio_head.load_state_dict(audio_state)\n",
    "audio_head.eval()\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "def create_mobilenetv3_tf_style(num_classes=2):\n",
    "\n",
    "    model = models.mobilenet_v3_small(weights=None)\n",
    "    in_features = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "image_model = create_mobilenetv3_tf_style(num_classes=2)\n",
    "\n",
    "img_state = torch.load(IMAGE_MODEL_PATH, map_location=DEVICE)\n",
    "image_model.load_state_dict(img_state)\n",
    "\n",
    "image_model.to(DEVICE)\n",
    "image_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "115192c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_probs(loader):\n",
    "    all_probs = []\n",
    "    text_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
    "            outputs = text_model(input_ids=input_ids, attention_mask=attn)\n",
    "            logits = outputs.logits\n",
    "            probs = softmax(logits, dim=-1)[:, 0]  # prob hoax (kelas 0)\n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "    return np.array(all_probs)\n",
    "\n",
    "def get_audio_probs(loader):\n",
    "    all_probs = []\n",
    "    audio_head.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            emb = batch[\"emb\"].to(DEVICE)\n",
    "            logits = audio_head(emb)\n",
    "            probs = softmax(logits, dim=-1)[:, 0]  # prob hoax\n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "    return np.array(all_probs)\n",
    "\n",
    "def get_image_probs(loader):\n",
    "    all_probs = []\n",
    "    image_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            imgs = batch[\"image\"].to(DEVICE)\n",
    "            logits = image_model(imgs)\n",
    "            probs = softmax(logits, dim=-1)[:, 0]  # prob hoax\n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "    return np.array(all_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "889995c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "p_text_train  = get_text_probs(train_text_loader)\n",
    "p_audio_train = get_audio_probs(train_audio_loader)\n",
    "p_img_train   = get_image_probs(train_img_loader)\n",
    "\n",
    "# Val\n",
    "p_text_val  = get_text_probs(val_text_loader)\n",
    "p_audio_val = get_audio_probs(val_audio_loader)\n",
    "p_img_val   = get_image_probs(val_img_loader)\n",
    "\n",
    "# Test\n",
    "p_text_test  = get_text_probs(test_text_loader)\n",
    "p_audio_test = get_audio_probs(test_audio_loader)\n",
    "p_img_test   = get_image_probs(test_img_loader)\n",
    "\n",
    "y_train = train_df[\"label_int\"].to_numpy()\n",
    "y_val   = val_df[\"label_int\"].to_numpy()\n",
    "y_test  = test_df[\"label_int\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "998d16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_probs(p_hoax, y_true, thr=0.5):\n",
    "    # label_int: 0=hoax,1=valid â†’ pred 0 jika prob hoax>=thr, else 1\n",
    "    y_pred = np.where(p_hoax >= thr, 0, 1)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_hoax = f1_score(y_true, y_pred, pos_label=0)\n",
    "    f1_valid = f1_score(y_true, y_pred, pos_label=1)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return acc, f1_hoax, f1_valid, f1_macro\n",
    "\n",
    "def fusion_two(p_a, p_b, alpha):\n",
    "    return alpha * p_a + (1 - alpha) * p_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "611f6928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alpha_text       acc   f1_hoax  f1_valid  f1_macro\n",
      "1         0.1  0.903226  0.800000  0.936170  0.868085\n",
      "0         0.0  0.806452  0.666667  0.863636  0.765152\n",
      "2         0.2  0.838710  0.615385  0.897959  0.756672\n",
      "4         0.4  0.838710  0.615385  0.897959  0.756672\n",
      "5         0.5  0.806452  0.571429  0.875000  0.723214\n",
      "Best alpha_text (text+audio): 0.1\n"
     ]
    }
   ],
   "source": [
    "alphas = np.linspace(0, 1, 11)\n",
    "\n",
    "records_ta = []\n",
    "for a in alphas:\n",
    "    p_ta_val = fusion_two(p_text_val, p_audio_val, a)\n",
    "    acc, f1h, f1v, f1m = evaluate_probs(p_ta_val, y_val)\n",
    "    records_ta.append((a, acc, f1h, f1v, f1m))\n",
    "\n",
    "df_ta = pd.DataFrame(records_ta, columns=[\"alpha_text\", \"acc\", \"f1_hoax\", \"f1_valid\", \"f1_macro\"])\n",
    "print(df_ta.sort_values(\"f1_macro\", ascending=False).head())\n",
    "\n",
    "best_row_ta = df_ta.sort_values(\"f1_macro\", ascending=False).iloc[0]\n",
    "alpha_text_best = float(best_row_ta[\"alpha_text\"])\n",
    "print(\"Best alpha_text (text+audio):\", alpha_text_best)\n",
    "\n",
    "# p_ta untuk semua split\n",
    "p_ta_train = fusion_two(p_text_train, p_audio_train, alpha_text_best)\n",
    "p_ta_val   = fusion_two(p_text_val,  p_audio_val,  alpha_text_best)\n",
    "p_ta_test  = fusion_two(p_text_test, p_audio_test, alpha_text_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e71b0d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    beta_ta       acc   f1_hoax  f1_valid  f1_macro\n",
      "10      1.0  0.903226  0.800000  0.936170  0.868085\n",
      "9       0.9  0.870968  0.714286  0.916667  0.815476\n",
      "0       0.0  0.774194  0.363636  0.862745  0.613191\n",
      "1       0.1  0.774194  0.363636  0.862745  0.613191\n",
      "2       0.2  0.774194  0.363636  0.862745  0.613191\n",
      "Best beta_ta (text+audio vs img): 1.0\n"
     ]
    }
   ],
   "source": [
    "betas = np.linspace(0, 1, 11)\n",
    "\n",
    "records_tai = []\n",
    "for b in betas:\n",
    "    p_tai_val = fusion_two(p_ta_val, p_img_val, b)  # b untuk p_ta, (1-b) untuk img\n",
    "    acc, f1h, f1v, f1m = evaluate_probs(p_tai_val, y_val)\n",
    "    records_tai.append((b, acc, f1h, f1v, f1m))\n",
    "\n",
    "df_tai = pd.DataFrame(records_tai, columns=[\"beta_ta\", \"acc\", \"f1_hoax\", \"f1_valid\", \"f1_macro\"])\n",
    "print(df_tai.sort_values(\"f1_macro\", ascending=False).head())\n",
    "\n",
    "best_row_tai = df_tai.sort_values(\"f1_macro\", ascending=False).iloc[0]\n",
    "beta_ta_best = float(best_row_tai[\"beta_ta\"])\n",
    "print(\"Best beta_ta (text+audio vs img):\", beta_ta_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5190bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Results ===\n",
      "Text        : acc=0.7188 f1h=0.4000 f1v=0.8163 f1m=0.6082\n",
      "Audio       : acc=0.8125 f1h=0.6250 f1v=0.8750 f1m=0.7500\n",
      "Image       : acc=0.6875 f1h=0.4444 f1v=0.7826 f1m=0.6135\n",
      "Text+Audio  : acc=0.8125 f1h=0.5714 f1v=0.8800 f1m=0.7257\n",
      "T+A+Image   : acc=0.8125 f1h=0.5714 f1v=0.8800 f1m=0.7257\n",
      "\n",
      "Classification report (T+A+Image):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.5000    0.5714         8\n",
      "           1     0.8462    0.9167    0.8800        24\n",
      "\n",
      "    accuracy                         0.8125        32\n",
      "   macro avg     0.7564    0.7083    0.7257        32\n",
      "weighted avg     0.8013    0.8125    0.8029        32\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 4  4]\n",
      " [ 2 22]]\n"
     ]
    }
   ],
   "source": [
    "# Text-only\n",
    "acc_t, f1h_t, f1v_t, f1m_t = evaluate_probs(p_text_test, y_test)\n",
    "\n",
    "# Audio-only\n",
    "acc_a, f1h_a, f1v_a, f1m_a = evaluate_probs(p_audio_test, y_test)\n",
    "\n",
    "# Image-only\n",
    "acc_i, f1h_i, f1v_i, f1m_i = evaluate_probs(p_img_test, y_test)\n",
    "\n",
    "# Text+Audio\n",
    "acc_ta, f1h_ta, f1v_ta, f1m_ta = evaluate_probs(p_ta_test, y_test)\n",
    "\n",
    "# Text+Audio+Image\n",
    "p_tai_test = fusion_two(p_ta_test, p_img_test, beta_ta_best)\n",
    "acc_tai, f1h_tai, f1v_tai, f1m_tai = evaluate_probs(p_tai_test, y_test)\n",
    "\n",
    "print(\"=== Test Results ===\")\n",
    "print(\"Text        : acc={:.4f} f1h={:.4f} f1v={:.4f} f1m={:.4f}\".format(acc_t,  f1h_t,  f1v_t,  f1m_t))\n",
    "print(\"Audio       : acc={:.4f} f1h={:.4f} f1v={:.4f} f1m={:.4f}\".format(acc_a,  f1h_a,  f1v_a,  f1m_a))\n",
    "print(\"Image       : acc={:.4f} f1h={:.4f} f1v={:.4f} f1m={:.4f}\".format(acc_i,  f1h_i,  f1v_i,  f1m_i))\n",
    "print(\"Text+Audio  : acc={:.4f} f1h={:.4f} f1v={:.4f} f1m={:.4f}\".format(acc_ta, f1h_ta, f1v_ta, f1m_ta))\n",
    "print(\"T+A+Image   : acc={:.4f} f1h={:.4f} f1v={:.4f} f1m={:.4f}\".format(acc_tai, f1h_tai, f1v_tai, f1m_tai))\n",
    "\n",
    "print(\"\\nClassification report (T+A+Image):\")\n",
    "y_pred_tai = np.where(p_tai_test >= 0.5, 0, 1)\n",
    "print(classification_report(y_test, y_pred_tai, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_tai))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cd9b08eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "class TextImageAudioLateFusion:\n",
    "    def __init__(self, alpha_text=0.0, beta_ta=1.0):\n",
    "        \"\"\"\n",
    "        alpha_text : bobot teks dalam fusion text+audio\n",
    "        beta_ta    : bobot (text+audio) vs image\n",
    "        \"\"\"\n",
    "        self.alpha_text = float(alpha_text)\n",
    "        self.alpha_audio = 1.0 - float(alpha_text)\n",
    "        self.beta_ta = float(beta_ta)\n",
    "\n",
    "    def fusion_text_audio(self, p_text_hoax, p_audio_hoax):\n",
    "        # p_*_hoax: array shape (N,) berisi prob hoax\n",
    "        return self.alpha_text * p_text_hoax + self.alpha_audio * p_audio_hoax\n",
    "\n",
    "    def fusion_all(self, p_text_hoax, p_audio_hoax, p_img_hoax):\n",
    "        \"\"\"\n",
    "        Menghasilkan probabilitas [p_hoax, p_valid] untuk 3 modal.\n",
    "        \"\"\"\n",
    "        p_ta = self.fusion_text_audio(p_text_hoax, p_audio_hoax)\n",
    "        p_hoax = self.beta_ta * p_ta + (1.0 - self.beta_ta) * p_img_hoax\n",
    "        p_valid = 1.0 - p_hoax\n",
    "        return np.vstack([p_hoax, p_valid]).T   # shape (N, 2)\n",
    "\n",
    "    def predict(self, p_text_hoax, p_audio_hoax, p_img_hoax, thr=0.5):\n",
    "        probs = self.fusion_all(p_text_hoax, p_audio_hoax, p_img_hoax)\n",
    "        # label: 0 = hoax, 1 = valid\n",
    "        return (probs[:, 0] < thr).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3e884314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3-modal fusion model to: D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\models\\fusion_final\\text_image_audio_late_fusion_a0.10_b1.00.joblib\n"
     ]
    }
   ],
   "source": [
    "fusion_tai = TextImageAudioLateFusion(\n",
    "    alpha_text=alpha_text_best,\n",
    "    beta_ta=beta_ta_best,\n",
    ")\n",
    "\n",
    "fname = rf\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\models\\fusion_final\\text_image_audio_late_fusion_a{alpha_text_best:.2f}_b{beta_ta_best:.2f}.joblib\"\n",
    "\n",
    "joblib.dump(fusion_tai, fname)\n",
    "print(\"Saved 3-modal fusion model to:\", fname)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
