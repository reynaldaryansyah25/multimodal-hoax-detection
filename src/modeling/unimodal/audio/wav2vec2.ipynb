{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46012268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cuda\n"
     ]
    }
   ],
   "source": [
    "# Config dasar\n",
    "import os, random, math, gc\n",
    "import numpy as np, pandas as pd, torch\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_CSV = r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\data\\training\\multimodal_splits\\audio_only_dataset.csv\"\n",
    "AUDIO_BASE_DIR = r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\"  \n",
    "MODEL_SAVE_PATH = r\"D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\models\\audio_baseline/audio_head_improved.pt\"\n",
    "EMB_SAVE = \"audio_embeddings_precomputed.npz\"\n",
    "TARGET_SR = 16000\n",
    "SEGMENT_SEC = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE =\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce42ee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 208\n",
      "                              audio_path  \\\n",
      "0  ./data/raw/youtube/audio/YT_00081.wav   \n",
      "1  ./data/raw/youtube/audio/YT_00867.wav   \n",
      "2  ./data/raw/youtube/audio/YT_00904.wav   \n",
      "3  ./data/raw/youtube/audio/YT_01161.wav   \n",
      "4  ./data/raw/youtube/audio/YT_00039.wav   \n",
      "\n",
      "                                     audio_path_full  \n",
      "0  D:\\INDONERIS-DATAMINING\\multimodal-hoax-detect...  \n",
      "1  D:\\INDONERIS-DATAMINING\\multimodal-hoax-detect...  \n",
      "2  D:\\INDONERIS-DATAMINING\\multimodal-hoax-detect...  \n",
      "3  D:\\INDONERIS-DATAMINING\\multimodal-hoax-detect...  \n",
      "4  D:\\INDONERIS-DATAMINING\\multimodal-hoax-detect...  \n",
      "D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\data/raw/youtube/audio/YT_00081.wav True\n",
      "D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\data/raw/youtube/audio/YT_00867.wav True\n",
      "D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\data/raw/youtube/audio/YT_00904.wav True\n",
      "D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\data/raw/youtube/audio/YT_01161.wav True\n",
      "D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\data/raw/youtube/audio/YT_00039.wav True\n"
     ]
    }
   ],
   "source": [
    "# Load CSV dan buat kolom path full\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "def to_full_path(rel_path):\n",
    "    import pandas as pd\n",
    "    if pd.isna(rel_path): return \"\"\n",
    "    p = str(rel_path).lstrip(\"./\")\n",
    "    return os.path.join(AUDIO_BASE_DIR, p)\n",
    "df[\"audio_path_full\"] = df[\"audio_path\"].apply(to_full_path)\n",
    "print(\"rows:\", len(df))\n",
    "print(df[[\"audio_path\",\"audio_path_full\"]].head(5))\n",
    "# quick existence check (first 5)\n",
    "for p in df[\"audio_path_full\"].head(5):\n",
    "    print(p, os.path.exists(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8269ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader: center segment, resample, mono, VAD check, optional spectral gating denoise\n",
    "import librosa, soundfile as sf\n",
    "\n",
    "def load_center_segment_waveform(path, target_sr=TARGET_SR, segment_sec=SEGMENT_SEC):\n",
    "    import torchaudio, torch.nn as nn\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if sr != target_sr:\n",
    "        wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "    num_samples = wav.shape[1]\n",
    "    seg_len = int(target_sr * segment_sec)\n",
    "    if num_samples <= seg_len:\n",
    "        pad_len = seg_len - num_samples\n",
    "        wav = nn.functional.pad(wav, (0, pad_len))\n",
    "    else:\n",
    "        center = num_samples // 2\n",
    "        start = max(0, center - seg_len // 2)\n",
    "        end = start + seg_len\n",
    "        wav = wav[:, start:end]\n",
    "    arr = wav.squeeze(0).numpy()\n",
    "    return arr\n",
    "\n",
    "def is_voiced_enough(y, sr=TARGET_SR, top_db=25, min_voiced_sec=0.5):\n",
    "    intervals = librosa.effects.split(y, top_db=top_db)\n",
    "    voiced_samples = sum((end-start) for start,end in intervals)\n",
    "    return (voiced_samples / sr) >= min_voiced_sec\n",
    "\n",
    "# optional spectral gating denoise (very light)\n",
    "def spectral_gate(y, sr, prop_decrease=0.9):\n",
    "    import numpy as np, scipy.signal\n",
    "    # simple spectral gating using short-time magnitude threshold (fast)\n",
    "    S = librosa.stft(y, n_fft=1024, hop_length=256)\n",
    "    mag = np.abs(S)\n",
    "    med = np.median(mag, axis=1, keepdims=True)\n",
    "    mask = mag >= med * 1.0\n",
    "    S2 = S * mask\n",
    "    y2 = librosa.istft(S2)\n",
    "    return y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "595321e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding file exists, skip precompute: audio_embeddings_precomputed.npz\n"
     ]
    }
   ],
   "source": [
    "# Precompute wav2vec embeddings to disk (one-time). Jika sudah ada file EMB_SAVE, skip.\n",
    "if os.path.exists(EMB_SAVE):\n",
    "    print(\"Embedding file exists, skip precompute:\", EMB_SAVE)\n",
    "else:\n",
    "    from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "    import torch\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "    model_wv = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(DEVICE)\n",
    "    model_wv.eval()\n",
    "\n",
    "    ids = []\n",
    "    paths = []\n",
    "    embeddings = []\n",
    "    failed = []\n",
    "    from tqdm.auto import tqdm\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        pid = row.get(\"sample_id\", str(i))\n",
    "        p = row[\"audio_path_full\"]\n",
    "        ids.append(pid); paths.append(p)\n",
    "        try:\n",
    "            y = load_center_segment_waveform(p)\n",
    "            if not is_voiced_enough(y):\n",
    "                embeddings.append(np.zeros((1,768), dtype=np.float32))\n",
    "                continue\n",
    "            # optional denoise: uncomment if needed (slow)\n",
    "            # y = spectral_gate(y, TARGET_SR)\n",
    "            inputs = processor(y, sampling_rate=TARGET_SR, return_tensors=\"pt\", padding=True)\n",
    "            with torch.no_grad():\n",
    "                out = model_wv(inputs.input_values.to(DEVICE))\n",
    "            hid = out.last_hidden_state.cpu().numpy()  # (1, L, D)\n",
    "            embeddings.append(hid.squeeze(0).astype(np.float32))\n",
    "        except Exception as e:\n",
    "            failed.append((p, str(e)))\n",
    "            embeddings.append(np.zeros((1,768), dtype=np.float32))\n",
    "    np.savez_compressed(EMB_SAVE, ids=np.array(ids), paths=np.array(paths), embeddings=embeddings)\n",
    "    print(\"Saved embeddings:\", EMB_SAVE, \"failures:\", len(failed))\n",
    "    del model_wv; torch.cuda.empty_cache(); gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c765fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset that reads precomputed embeddings (.npz)\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "data_np = np.load(EMB_SAVE, allow_pickle=True)\n",
    "emb_list = data_np[\"embeddings\"]\n",
    "paths_saved = data_np[\"paths\"].tolist()\n",
    "ids_saved = data_np[\"ids\"].tolist()\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, df, emb_list, label_map=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.emb = emb_list\n",
    "        self.label_map = label_map or {\"hoax\":0, \"valid\":1}\n",
    "    def __len__(self): return len(self.emb)\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.emb[idx].astype(np.float32)\n",
    "        label = self.df.iloc[idx][\"label\"]\n",
    "        if isinstance(label, str):\n",
    "            label = self.label_map.get(label, 0)\n",
    "        return {\"embeddings\": torch.tensor(e), \"label\": torch.tensor(int(label))}\n",
    "\n",
    "def collate_varbatch(batch):\n",
    "    emb_list = [item[\"embeddings\"] for item in batch]\n",
    "    labs = torch.stack([item[\"label\"] for item in batch])\n",
    "    max_len = max(e.shape[0] for e in emb_list); dim = emb_list[0].shape[1]\n",
    "    padded = torch.zeros(len(emb_list), max_len, dim, dtype=torch.float32)\n",
    "    for i,e in enumerate(emb_list): padded[i, :e.shape[0], :] = e\n",
    "    return {\"embeddings\": padded, \"label\": labs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26d17c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model head and focal loss\n",
    "import torch.nn as nn, torch\n",
    "\n",
    "class AttentivePool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.context = nn.Linear(dim, 1)\n",
    "    def forward(self, x):\n",
    "        h = torch.tanh(self.linear(x))\n",
    "        scores = self.context(h).squeeze(-1)\n",
    "        w = torch.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        return (w * x).sum(dim=1)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, dim=768, hidden=256, nclass=2):\n",
    "        super().__init__()\n",
    "        self.pool = AttentivePool(dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, nclass)\n",
    "        )\n",
    "    def forward(self, x): return self.fc(self.pool(x))\n",
    "\n",
    "# focal loss (binary/multi)\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma; self.alpha = alpha; self.reduction = reduction\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        targets_onehot = torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float()\n",
    "        p_t = (probs * targets_onehot).sum(dim=1)\n",
    "        loss = - self.alpha * (1 - p_t) ** self.gamma * torch.log(p_t + 1e-9)\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72183de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\.venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 0.6929\n",
      "epoch 2 loss 0.6749\n",
      "epoch 3 loss 0.6704\n",
      "epoch 4 loss 0.6577\n",
      "epoch 5 loss 0.6467\n",
      "epoch 6 loss 0.6270\n",
      "Saved head to D:\\INDONERIS-DATAMINING\\multimodal-hoax-detection\\models\\audio_baseline/audio_head_improved.pt\n"
     ]
    }
   ],
   "source": [
    "# Build dataloader, compute class weights, train head only (fast)\n",
    "ds = EmbeddingDataset(df, emb_list)\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=True, collate_fn=collate_varbatch, num_workers=0)\n",
    "\n",
    "# class weights\n",
    "from collections import Counter\n",
    "cnt = Counter(df[\"label\"].astype(str).tolist())\n",
    "# map to numeric labels used earlier\n",
    "lab_map = {\"hoax\":0, \"valid\":1}\n",
    "counts = [cnt.get(\"hoax\",0), cnt.get(\"valid\",0)]\n",
    "total = sum(counts)\n",
    "class_weights = [total/(c+1e-9) if c>0 else 1.0 for c in counts]\n",
    "class_weights = torch.tensor(class_weights).to(DEVICE)\n",
    "\n",
    "model = Head(dim=768).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose=True)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "# or use focal: criterion = FocalLoss(gamma=2.0, alpha=0.25)\n",
    "\n",
    "for epoch in range(6):\n",
    "    model.train()\n",
    "    total_loss=0.0; n=0\n",
    "    for b in loader:\n",
    "        emb = b[\"embeddings\"].to(DEVICE)\n",
    "        labels = b[\"label\"].to(DEVICE)\n",
    "        logits = model(emb)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        total_loss += loss.item(); n+=1\n",
    "    avg = total_loss / max(1,n)\n",
    "    print(f\"epoch {epoch+1} loss {avg:.4f}\")\n",
    "    scheduler.step(avg)\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(\"Saved head to\", MODEL_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97e26f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hoax     0.5424    0.6154    0.5766        52\n",
      "       valid     0.8658    0.8269    0.8459       156\n",
      "\n",
      "    accuracy                         0.7740       208\n",
      "   macro avg     0.7041    0.7212    0.7112       208\n",
      "weighted avg     0.7849    0.7740    0.7786       208\n",
      "\n",
      "accuracy: 0.7740384615384616\n",
      "confusion matrix:\n",
      " [[ 32  20]\n",
      " [ 27 129]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and show classification report + confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_pretty(model, df, emb_list, batch_size=8, label_names=[\"hoax\",\"valid\"]):\n",
    "    ds = EmbeddingDataset(df, emb_list)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_varbatch, num_workers=0)\n",
    "    model.to(DEVICE); model.eval()\n",
    "    y_true=[]; y_pred=[]; y_prob=[]\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            emb = b[\"embeddings\"].to(DEVICE)\n",
    "            labels = b[\"label\"].cpu().numpy()\n",
    "            logits = model(emb).cpu().numpy()\n",
    "            exp = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "            probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            y_true.extend(labels.tolist()); y_pred.extend(preds.tolist()); y_prob.extend(probs.tolist())\n",
    "    print(classification_report(y_true, y_pred, target_names=label_names, digits=4))\n",
    "    print(\"accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# usage:\n",
    "evaluate_pretty(model, df, emb_list, batch_size=8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
